{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Identification by Gait on the Gotcha Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Tasks:\n",
    "1) Extract frames from video, we don’t need all the frames but samples of consecutive\n",
    "frames<br>\n",
    "2) Use Real time Multi-Person 2D Pose Estimation Using Part Affinity Fields to detect the\n",
    "skeleton of the subject in the frame, particularly his feature body points<br>\n",
    "3) Store the position/coordinates of consecutive frame feature points in an array to create\n",
    "the pattern array (the pattern array store informations about the subject movement\n",
    "variations in time)<br>\n",
    "4) Create a NN that use as input the pattern array and give us the user id as output:<br>\n",
    "5) Use a lot of pattern array samples to train the network<br>\n",
    "6) Try to reach the highest accuracy chosing the best loss function.<br>\n",
    "7) Additionally classify if each video is inside, outside or with a flashlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Content Description:\n",
    "Content of the Dataset, Folders:<br>\n",
    "Inside each folder there are two numbers: first number is the number of the folder and the second number is the subject's ID. <br>\n",
    "1_Indoor lights - cooperative  = there are videos of 62 subjects indoor and in cooperative way.<br> \n",
    "2_Indoor light - non cooperative = there are videos of 62 subjects indoor and in cooperative way. <br>\n",
    "3_Indoor flash - cooperative  = there are videos of 62 subjects indoor with light on and flash on, in cooperative way.<br> \n",
    "4_Indoor flash - non cooperative = there are videos of 62 subjects indoor with light on and flash on, in non cooperative way. <br>\n",
    "5_Outdoor - cooperative  = there are videos of 62 subjects outdoor in cooperative way. <br>\n",
    "6_Outdoor - non cooperative = there are videos of 62 subjects outdoor in non cooperative way. <br>\n",
    "7_180 head video =  there are videos of the face in 180¡ (from ear to ear) of 62 subjects.<br>\n",
    "8_Stairs - cooperative = there are videos of 6 subjects climbing the stairs in 3 different angles with 3 different devices (A-B-C), in cooperative way.<br>\n",
    "9_Stairs - non cooperative =  there are videos of 6 subjects climbing the stairs in 3 different angles with 3 different devices (A-B-C), in non cooperative way.<br>\n",
    "10_Outdoor path - cooperative = there are videos of 12 subjects walking outdoor along a path in cooperative way. (iPhone)<br>\n",
    "11_Outdoor path - non cooperative = there are videos of 12 subjects walking outdoor along a path in non cooperative way. <br>(iPhone)\n",
    "12_Derived file attached = there are two tipes of folders (1) HPE_x where x = 1, 2, 3, ... 62.  (2) Landmarks<br>\n",
    "\n",
    "(1) HPE_x folder contains the 3D model of the subject x and 2223 images with the head rotated in pitch, yaw and roll<br>\n",
    "(2) Landmarks folder contains Body landmarks and face landmarks of 62 subjects in the videos 1, 2, 3, 4, 5, 6.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Extracting the Frames from the Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with this cell, I will always put one explanatory markdown cell on top of each code block, in addition to the comments in each cell. First I install all the necessary dependencies and display the current working path of the notebook.\n",
    "The first line is to display matplotlib plots in line with the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is C:\\Users\\marvi\\Desktop\\UNISA\\CV Project\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd                                   # for working with large dataframes\n",
    "import numpy as np                                    # for working with efficient numpy arrays\n",
    "import os                                             # for operating system\n",
    "import shutil                                         # for more utilities\n",
    "import csv                                            # for reading excel tables and similar files\n",
    "import matplotlib.pyplot as plt                       # for plotting the images\n",
    "import cv2                                            # for capturing videos\n",
    "import math                                           # for mathematical operations\n",
    "from keras.preprocessing import image                 # for preprocessing the images\n",
    "from keras.utils import np_utils                      # some utilities\n",
    "from skimage.transform import resize                  # for resizing images\n",
    "from scipy import ndimage                             # for rotating images\n",
    "from config_reader import config_reader               # for 2D pose estimation\n",
    "from model.cmu_model import get_testing_model         # for loading the affinity model\n",
    "import time                                           # capturing the time needed for some operations\n",
    "from tqdm import tqdm_notebook as tqdm                # having progress bars for loops\n",
    "from sklearn.model_selection import train_test_split  # splitting training and test data\n",
    "from sklearn.preprocessing import OneHotEncoder       # Encoding labels to vectors\n",
    "from keras.utils import to_categorical                # Encoding Labels to numbers\n",
    "from keras.models import Sequential                   # Building network architecture\n",
    "from keras.layers import Dense, BatchNormalization, LSTM, Conv2D, Conv3D, Flatten    # Neural Network Layers\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "wpath = os.getcwd()\n",
    "print (\"The current working directory is %s\" % wpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I declare a function to extact the images. Because the walking of the subjects mostly appears in the middle of each video, I start extracting frames specifically after a third of the video is over and extract 32 consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract frames from the videos\n",
    "def vid2image(filename, saveTo):\n",
    "    \n",
    "    count = 0\n",
    "    frameList = []\n",
    "\n",
    "    cap = cv2.VideoCapture(filename)                    # capturing the video from the given path\n",
    "    frameRate = cap.get(5)                              # frame rate\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))     #get the total frame count\n",
    "\n",
    "    if(frameRate == 0):\n",
    "        print(\"File could not be loaded\")               #in case the file is corrupted\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1)                            #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        \n",
    "        if(frameId in range(int(round(length/3)),int(round(length/3)+32))): #Extracting 32 frames from the middle of the video\n",
    "            filename2 = str(filename[-9:-4]) + \"_frame%%%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(os.path.join(saveTo , filename2), frame)\n",
    "                \n",
    "                \n",
    "    cap.release()\n",
    "    if(count==0):\n",
    "        print(\"No Frames Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only used the folders 1 to 6 and placed their videos in a shared folder from where I import all videos. The others had missing subjects and sometimes people in the background. After extracting the images once, I commented the function out, so I dont accidentally start everything again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d3360462654625b9a9447cb6f5a5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=372), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "372 Videos found\n",
      "0 Videos failed to load\n"
     ]
    }
   ],
   "source": [
    "path = './data/13_all_62_subjects/'\n",
    "path_out = './extracted_frames/'\n",
    "\n",
    "i = 0\n",
    "fail = 0\n",
    "X_data = []\n",
    "\n",
    "for filename in tqdm(os.listdir(path)):        \n",
    "\n",
    "    if('.mp4' in filename):\n",
    "        \n",
    "        # extracing the frame samples\n",
    "        i = i +1\n",
    "        label = filename[0:5]\n",
    "        X_data.append([filename[0],filename[2:5]])\n",
    "        #vid2image(path+filename, path_out) # remove the first # to load all videos again, this may take an hour\n",
    "        \n",
    "\n",
    "print(str(i) + \" Videos found\")\n",
    "print(str(fail) + \" Videos failed to load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step I resized all the extracted frames from 1920x1080 to 192x108, so the dimensions for the 2D Pose Estimation are greatly reduced but you can still see all necessary details in the images in order to identify body parts. Because a lot of the fotos were shot in vertical mode, I also rotate them into the correct position. While testing I noticed a great drop in performance of the 2d pose estimation algorithm, when the frames are flipped in a wrong direction. I assume its not really trained that way, so its not really robust against rotation. Usually the Pose Estimator missed around 40% of coordinates of almost all joints, when the image wasn't in correct rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd145d0e44954784988ad52721f5949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32 frames found\n"
     ]
    }
   ],
   "source": [
    "#resizing the frames\n",
    "n=0\n",
    "path_frames = './extracted_frames/'\n",
    "outpath = './lowres/'\n",
    "\n",
    "for filename in tqdm(os.listdir(path_frames)):\n",
    "    \n",
    "    n=n+1\n",
    "\n",
    "    if('frameX' in filename):                                                   #remove X to resize all frames in the folder\n",
    "        img = cv2.imread(path_frames+filename)\n",
    "        res = cv2.resize(img, dsize=(192, 108), interpolation=cv2.INTER_CUBIC)\n",
    "        res = np.rot90(res,k=3)                                                 #rotating images into correct position\n",
    "        cv2.imwrite(os.path.join(outpath, filename), res)\n",
    "    \n",
    "        \n",
    "print(str(n) + \" frames found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Performing 2D Pose Estimation and storing the Joint Positions\n",
    "I downloaded the weights of the 2D Pose Estimation Paper and here I load them into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the weighted 2d pose estimation model\n",
    "afmodel = get_testing_model()\n",
    "afmodel.load_weights('model/keras/model.h5')\n",
    "\n",
    "# load config\n",
    "params, model_params = config_reader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform 2D Pose Estimation multiple times, I put the model into a function, where you can also still comment out the draw operation when you don't want to save every skeleton with an image but just want to get the joint coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = []\n",
    "\n",
    "#function to perform 2d pose estimation on images\n",
    "def affinity(image_path, outpath):\n",
    "    \n",
    "        \"\"\"the affinity method takes an input image and performs 2D Pose Estimation on it.\n",
    "        It returns an array with all peaks where the coordinates of the found joints are stored. \n",
    "        If needed, it can also create a new image and mark all the found joint in it with points, \n",
    "        connecting joints that are neighbours. \"\"\"\n",
    "    \n",
    "        output = outpath\n",
    "        tic = time.time()\n",
    "        \n",
    "        input_image = cv2.imread(image_path)  # B,G,R order\n",
    "        \n",
    "        #extracting the affinity field heatmap peaks from the images\n",
    "        all_peaks, subset, candidate = extract_parts(input_image, params, afmodel, model_params)\n",
    "        canvas = draw(input_image, all_peaks, subset, candidate)\n",
    "        peaks.append(np.array(all_peaks))\n",
    "\n",
    "        toc = time.time()\n",
    "        print('processing time was %.5f' % (toc - tic))\n",
    "\n",
    "        cv2.imwrite(output, canvas)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While performing 2D Pose Estimation on every frame, each time the joint coordinates get written into a text file, while the images with the skeleton get saved in a separate folder. In each textfile, I removed the braces from the strings and replaced empty coordinate lists with the numpy nan, for managing missing data easier, later in the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dff20fb516b45f48376ad794e1745a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11744), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11744 frames found\n"
     ]
    }
   ],
   "source": [
    "path_frames = './lowres/'\n",
    "outpath = './affinity/'\n",
    "\n",
    "joint_data = []\n",
    "m=0\n",
    "\n",
    "for file in tqdm(sorted(os.listdir(path_frames))):\n",
    "    m+=1\n",
    "\n",
    "    if('frameX' in file):#remove X to cycle through all the lowres images to perform pose estimation\n",
    "        \n",
    "        peaks = []   \n",
    "        \n",
    "        #perform 2D Pose Estimation\n",
    "        affinity(path_frames+file, outpath+file)\n",
    "        \n",
    "        #save heatmap peaks of each joint in a text file\n",
    "        jdf = \"./joint_data/\"+ file[:-4] +\".txt\"\n",
    "        f = open(jdf,\"w+\")\n",
    "        \n",
    "        #removing braces, writing one line per joint , saving only XY coordinates\n",
    "        for frame in peaks:\n",
    "            for joint in frame:\n",
    "                f.write(\"\\n\")\n",
    "                position = str(joint)\n",
    "                position = position.replace(\"[]\",\"NaN    \")\n",
    "                position = position.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
    "                position = position[:-3]\n",
    "                pos_rounded = position.split()\n",
    "                for pos in pos_rounded:\n",
    "                    pos = (pos[:7]) if len(pos) > 7 else pos\n",
    "                    f.write(pos+\" \")\n",
    "\n",
    "                \n",
    "        f.close()\n",
    "\n",
    "    joint_data.append(peaks)    \n",
    "print(str(m) + \" frames found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second notebook I'm starting to preprocess the joint data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
